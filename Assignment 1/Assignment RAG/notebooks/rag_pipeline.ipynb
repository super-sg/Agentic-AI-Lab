{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ ArXiv Research Paper RAG Pipeline\n",
    "\n",
    "**Retrieval-Augmented Generation for AI/ML Research Papers**\n",
    "\n",
    "This notebook walks through building a complete RAG system that can answer questions about academic research papers using semantic search and LLM-powered generation.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "ğŸ“„ PDFs â†’ ğŸ“ Text Extraction â†’ âœ‚ï¸ Chunking â†’ ğŸ§® Embedding â†’ ğŸ“¦ FAISS Index\n",
    "                                                                    â†“\n",
    "                                    ğŸ¤– LLM Answer â† ğŸ“‹ Context â† ğŸ” Retrieval â† â“ Query\n",
    "```\n",
    "\n",
    "| Component | Choice | Justification |\n",
    "|-----------|--------|---------------|\n",
    "| Chunking | 512 tokens, 50 overlap | Captures 1-2 academic paragraphs; overlap prevents boundary loss |\n",
    "| Embedding | `all-MiniLM-L6-v2` (384d) | Best quality/speed ratio, 14 MB, trained on 1B+ pairs |\n",
    "| Vector DB | FAISS `IndexFlatIP` | Open-source, no server, exact inner product search |\n",
    "| LLM | Configurable (OpenAI/Ollama) | Flexible for cloud or local inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 1: Setup & Imports\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# Import our pipeline components\n",
    "from config import (\n",
    "    PAPERS_DIR, METADATA_FILE, FAISS_INDEX_DIR,\n",
    "    CHUNK_SIZE, CHUNK_OVERLAP, EMBEDDING_MODEL_NAME, TOP_K,\n",
    ")\n",
    "from rag_pipeline import (\n",
    "    PDFTextExtractor, TextChunker, EmbeddingEngine,\n",
    "    VectorStore, RAGPipeline,\n",
    ")\n",
    "\n",
    "# Configure display\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Papers dir:   {PAPERS_DIR}\")\n",
    "print(f\"âœ… Chunk size:   {CHUNK_SIZE} chars, {CHUNK_OVERLAP} overlap\")\n",
    "print(f\"âœ… Embed model:  {EMBEDDING_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Data Loading & Exploration\n",
    "\n",
    "Load the metadata JSON created by `collect_papers.py` and explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 2: Data Loading & Exploration\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Load metadata\n",
    "with open(METADATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(metadata)\n",
    "print(f\"ğŸ“Š Dataset Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total papers:       {len(df)}\")\n",
    "print(f\"Unique categories:  {df['primary_category'].nunique()}\")\n",
    "print(f\"Date range:         {df['published'].min()[:10]} to {df['published'].max()[:10]}\")\n",
    "print(f\"\\nğŸ“ Category Distribution:\")\n",
    "print(df['primary_category'].value_counts().to_string())\n",
    "\n",
    "# Show sample papers\n",
    "print(f\"\\nğŸ“„ Sample Papers:\")\n",
    "df[['title', 'primary_category', 'published']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: PDF Text Extraction\n",
    "\n",
    "Extract text from all downloaded PDFs using `pdfplumber` (primary) with `PyPDF2` fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 3: PDF Text Extraction\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "extractor = PDFTextExtractor()\n",
    "documents = extractor.extract_all(PAPERS_DIR, METADATA_FILE)\n",
    "\n",
    "print(f\"\\nğŸ“„ Text Extraction Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Successfully extracted: {len(documents)}/{len(metadata)} papers\")\n",
    "\n",
    "# Text length statistics\n",
    "text_lengths = [len(doc['text']) for doc in documents]\n",
    "print(f\"\\nğŸ“ Text Length Statistics (characters):\")\n",
    "print(f\"  Mean:   {np.mean(text_lengths):,.0f}\")\n",
    "print(f\"  Median: {np.median(text_lengths):,.0f}\")\n",
    "print(f\"  Min:    {np.min(text_lengths):,.0f}\")\n",
    "print(f\"  Max:    {np.max(text_lengths):,.0f}\")\n",
    "print(f\"  Total:  {sum(text_lengths):,.0f}\")\n",
    "\n",
    "# Show a sample extracted text\n",
    "sample = documents[0]\n",
    "print(f\"\\nğŸ“ Sample â€” '{sample['title'][:60]}...'\")\n",
    "print(f\"   First 500 chars:\")\n",
    "print(f\"   {sample['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Chunking Strategy\n",
    "\n",
    "Split documents into overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "**Why 512 tokens, 50 overlap?**\n",
    "- Academic paragraphs average 150â€“300 words (~200â€“400 tokens)\n",
    "- 512 tokens captures 1â€“2 complete paragraphs â€” enough context without dilution\n",
    "- 50-token overlap (~10%) prevents boundary information loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 4: Chunking Strategy\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "chunker = TextChunker(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "chunks = chunker.chunk_documents(documents)\n",
    "\n",
    "# Chunking statistics\n",
    "stats = chunker.get_stats(chunks)\n",
    "print(f\"\\nâœ‚ï¸  Chunking Results\")\n",
    "print(f\"{'='*50}\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key:25s}: {value:.1f}\")\n",
    "    else:\n",
    "        print(f\"  {key:25s}: {value}\")\n",
    "\n",
    "# Visualize chunk length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "chunk_lengths = [len(c['text']) for c in chunks]\n",
    "\n",
    "axes[0].hist(chunk_lengths, bins=50, color='#4C72B0', edgecolor='white', alpha=0.8)\n",
    "axes[0].axvline(x=CHUNK_SIZE, color='red', linestyle='--', label=f'Target: {CHUNK_SIZE}')\n",
    "axes[0].set_xlabel('Chunk Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Chunk Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Chunks per paper\n",
    "chunks_per_paper = {}\n",
    "for c in chunks:\n",
    "    chunks_per_paper[c['paper_id']] = chunks_per_paper.get(c['paper_id'], 0) + 1\n",
    "cpv = list(chunks_per_paper.values())\n",
    "\n",
    "axes[1].hist(cpv, bins=30, color='#55A868', edgecolor='white', alpha=0.8)\n",
    "axes[1].set_xlabel('Number of Chunks')\n",
    "axes[1].set_ylabel('Number of Papers')\n",
    "axes[1].set_title('Chunks per Paper Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / 'data' / 'chunk_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Show sample chunks from one paper\n",
    "sample_id = chunks[0]['paper_id']\n",
    "sample_chunks = [c for c in chunks if c['paper_id'] == sample_id]\n",
    "print(f\"\\nğŸ“‹ Sample chunks from '{sample_chunks[0]['paper_title'][:50]}...':\")\n",
    "for c in sample_chunks[:3]:\n",
    "    print(f\"\\n  Chunk {c['chunk_index']+1}/{c['total_chunks']} ({len(c['text'])} chars):\")\n",
    "    print(f\"  {c['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Embedding Generation\n",
    "\n",
    "Generate dense vector embeddings using `all-MiniLM-L6-v2`.\n",
    "\n",
    "**Model Details:**\n",
    "- 384 dimensions, ~22M parameters, ~14 MB\n",
    "- Trained on 1B+ sentence pairs (NLI + semantic similarity)\n",
    "- Maps text to a 384-dimensional dense vector space\n",
    "- L2-normalized â†’ inner product = cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 5: Embedding Generation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import time\n",
    "\n",
    "engine = EmbeddingEngine(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Embed all chunks\n",
    "texts = [c['text'] for c in chunks]\n",
    "\n",
    "start_time = time.time()\n",
    "embeddings = engine.embed_texts(texts, batch_size=64)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nğŸ§® Embedding Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Model:         {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"  Chunks:        {embeddings.shape[0]}\")\n",
    "print(f\"  Dimensions:    {embeddings.shape[1]}\")\n",
    "print(f\"  Time:          {elapsed:.1f}s ({len(texts)/elapsed:.0f} chunks/sec)\")\n",
    "print(f\"  Memory:        {embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  Dtype:         {embeddings.dtype}\")\n",
    "\n",
    "# Verify L2 normalization (norms should be ~1.0)\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(f\"\\n  L2 norms â€” mean: {norms.mean():.4f}, std: {norms.std():.6f}\")\n",
    "\n",
    "# Show a sample embedding vector\n",
    "print(f\"\\n  Sample embedding (first 10 dims):\")\n",
    "print(f\"  {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: FAISS Vector Store Creation\n",
    "\n",
    "Build a FAISS index for fast similarity search.\n",
    "\n",
    "**Why FAISS?**\n",
    "- Open-source (Meta AI), no external server needed\n",
    "- Extremely fast approximate nearest neighbor search\n",
    "- `IndexFlatIP` â€” exact inner product search (cosine sim for normalized vectors)\n",
    "- Perfectly scaled for our dataset (5Kâ€“50K chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 6: FAISS Index Creation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "store = VectorStore(dimension=engine.dimension)\n",
    "store.build_index(embeddings, chunks)\n",
    "\n",
    "# Save to disk\n",
    "store.save(FAISS_INDEX_DIR)\n",
    "\n",
    "# Print index statistics\n",
    "index_stats = store.get_stats()\n",
    "print(f\"\\nğŸ“¦ FAISS Index Statistics\")\n",
    "print(f\"{'='*50}\")\n",
    "for key, value in index_stats.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "print(f\"\\n  Index type:    {type(store.index).__name__}\")\n",
    "print(f\"  Saved to:      {FAISS_INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Query Engine â€” Semantic Search\n",
    "\n",
    "Test the retrieval component independently before wiring up the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 7: Query Engine â€” Semantic Search\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def semantic_search(query: str, top_k: int = TOP_K) -> list[dict]:\n",
    "    \"\"\"Search the FAISS index for the most relevant chunks.\"\"\"\n",
    "    query_embedding = engine.embed_query(query)\n",
    "    results = store.search(query_embedding, top_k=top_k)\n",
    "    return results\n",
    "\n",
    "\n",
    "def display_results(query: str, results: list[dict]):\n",
    "    \"\"\"Pretty-print search results.\"\"\"\n",
    "    print(f\"\\nğŸ” Query: \\\"{query}\\\"\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n  [{i}] Score: {r['score']:.4f}\")\n",
    "        print(f\"      Paper: {r['paper_title'][:70]}\")\n",
    "        print(f\"      Chunk: {r['chunk_index']+1}/{r['total_chunks']}\")\n",
    "        print(f\"      Text:  {r['text'][:200]}...\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are transformer architectures and how do they work?\",\n",
    "    \"How does attention mechanism improve neural networks?\",\n",
    "    \"What are the latest advances in computer vision?\",\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    results = semantic_search(q, top_k=3)\n",
    "    display_results(q, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: RAG Chain with LLM\n",
    "\n",
    "Wire the retrieval engine into a full RAG chain with LLM-powered generation.\n",
    "\n",
    "> **Note:** Set `OPENAI_API_KEY` in your `.env` file, or start Ollama locally. If no LLM is configured, the system will return raw retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 8: RAG Chain with LLM\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load OPENAI_API_KEY from .env if present\n",
    "\n",
    "# Create the LLM\n",
    "llm = RAGPipeline.create_llm()\n",
    "\n",
    "# Build the full RAG pipeline\n",
    "rag = RAGPipeline(\n",
    "    embedding_engine=engine,\n",
    "    vector_store=store,\n",
    "    llm=llm,\n",
    "    top_k=TOP_K,\n",
    ")\n",
    "\n",
    "print(f\"âœ… RAG Pipeline initialized\")\n",
    "print(f\"   LLM: {'Configured âœ“' if llm else 'None (will return raw context) âš ï¸'}\")\n",
    "print(f\"   Top-K: {TOP_K}\")\n",
    "\n",
    "# Test the pipeline\n",
    "result = rag.query(\"What are the key innovations in transformer architectures?\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"â“ Question: {result['question']}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nğŸ’¬ Answer:\\n{result['answer'][:1000]}\")\n",
    "print(f\"\\nğŸ“š Sources ({result['num_sources']}):\")\n",
    "for s in result['sources']:\n",
    "    print(f\"   [{s['score']:.4f}] {s['paper_title'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Test Queries\n",
    "\n",
    "Run a diverse set of research questions to evaluate the RAG system's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 9: Test Queries\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "test_questions = [\n",
    "    # Architecture questions\n",
    "    \"What are the main types of neural network architectures used in NLP?\",\n",
    "    \n",
    "    # Comparison questions\n",
    "    \"How do CNNs compare to transformers for image classification tasks?\",\n",
    "    \n",
    "    # Methodology questions\n",
    "    \"What training techniques are used to improve model generalization?\",\n",
    "    \n",
    "    # Application questions\n",
    "    \"What are the real-world applications of reinforcement learning?\",\n",
    "    \n",
    "    # Recent advances\n",
    "    \"What are the latest developments in large language models?\",\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'â•'*70}\")\n",
    "    print(f\"  Query {i}: {question}\")\n",
    "    print(f\"{'â•'*70}\")\n",
    "    \n",
    "    result = rag.query(question, top_k=5)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Show answer (truncated)\n",
    "    answer_preview = result['answer'][:500]\n",
    "    print(f\"\\nğŸ’¬ Answer:\\n{answer_preview}{'...' if len(result['answer']) > 500 else ''}\")\n",
    "    \n",
    "    # Show sources\n",
    "    print(f\"\\nğŸ“š Top Sources:\")\n",
    "    for s in result['sources'][:3]:\n",
    "        print(f\"   [{s['score']:.4f}] {s['paper_title'][:60]}\")\n",
    "\n",
    "print(f\"\\n\\nâœ… Completed {len(test_questions)} test queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Evaluation & Metrics\n",
    "\n",
    "Evaluate the retrieval quality with quantitative metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 10: Evaluation & Metrics\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import time\n",
    "\n",
    "eval_queries = [\n",
    "    \"transformer architecture self-attention mechanism\",\n",
    "    \"convolutional neural networks image recognition\",\n",
    "    \"natural language processing text generation\",\n",
    "    \"reinforcement learning policy optimization\",\n",
    "    \"generative adversarial networks image synthesis\",\n",
    "    \"transfer learning domain adaptation\",\n",
    "    \"graph neural networks node classification\",\n",
    "    \"federated learning privacy preservation\",\n",
    "]\n",
    "\n",
    "# Metric 1: Retrieval Latency\n",
    "latencies = []\n",
    "for q in eval_queries:\n",
    "    start = time.time()\n",
    "    _ = semantic_search(q, top_k=5)\n",
    "    latencies.append((time.time() - start) * 1000)  # ms\n",
    "\n",
    "# Metric 2: Score Distribution\n",
    "all_scores = []\n",
    "all_top1_scores = []\n",
    "for q in eval_queries:\n",
    "    results = semantic_search(q, top_k=10)\n",
    "    scores = [r['score'] for r in results]\n",
    "    all_scores.extend(scores)\n",
    "    all_top1_scores.append(scores[0] if scores else 0)\n",
    "\n",
    "# Metric 3: Source Diversity\n",
    "diversity_scores = []\n",
    "for q in eval_queries:\n",
    "    results = semantic_search(q, top_k=5)\n",
    "    unique_papers = len(set(r['paper_id'] for r in results))\n",
    "    diversity_scores.append(unique_papers / max(len(results), 1))\n",
    "\n",
    "# Print metrics\n",
    "print(f\"ğŸ“Š Retrieval Evaluation Metrics\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nâ±ï¸  Latency (ms):\")\n",
    "print(f\"   Mean:   {np.mean(latencies):.1f}\")\n",
    "print(f\"   Median: {np.median(latencies):.1f}\")\n",
    "print(f\"   P95:    {np.percentile(latencies, 95):.1f}\")\n",
    "print(f\"   Max:    {np.max(latencies):.1f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Similarity Scores (cosine):\")\n",
    "print(f\"   Top-1 avg:    {np.mean(all_top1_scores):.4f}\")\n",
    "print(f\"   Overall avg:  {np.mean(all_scores):.4f}\")\n",
    "print(f\"   Overall std:  {np.std(all_scores):.4f}\")\n",
    "print(f\"   Score range:  [{np.min(all_scores):.4f}, {np.max(all_scores):.4f}]\")\n",
    "\n",
    "print(f\"\\nğŸŒ Source Diversity (unique papers / top-k):\")\n",
    "print(f\"   Mean:   {np.mean(diversity_scores):.2f}\")\n",
    "print(f\"   Min:    {np.min(diversity_scores):.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].bar(range(len(latencies)), latencies, color='#4C72B0', alpha=0.8)\n",
    "axes[0].axhline(y=np.mean(latencies), color='red', linestyle='--', label=f'Mean: {np.mean(latencies):.1f}ms')\n",
    "axes[0].set_xlabel('Query Index')\n",
    "axes[0].set_ylabel('Latency (ms)')\n",
    "axes[0].set_title('Retrieval Latency per Query')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(all_scores, bins=30, color='#55A868', edgecolor='white', alpha=0.8)\n",
    "axes[1].set_xlabel('Cosine Similarity Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Score Distribution (All Retrieved)')\n",
    "\n",
    "axes[2].bar(range(len(diversity_scores)), diversity_scores, color='#C44E52', alpha=0.8)\n",
    "axes[2].set_xlabel('Query Index')\n",
    "axes[2].set_ylabel('Diversity Score')\n",
    "axes[2].set_title('Source Diversity per Query')\n",
    "axes[2].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / 'data' / 'evaluation_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Visualization & Analysis\n",
    "\n",
    "Visualize the embedding space using t-SNE dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 11: Visualization & Analysis\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample embeddings for visualization (use all if < 2000, else sample)\n",
    "max_vis = 2000\n",
    "if len(embeddings) > max_vis:\n",
    "    indices = np.random.choice(len(embeddings), max_vis, replace=False)\n",
    "    vis_embeddings = embeddings[indices]\n",
    "    vis_chunks = [chunks[i] for i in indices]\n",
    "else:\n",
    "    vis_embeddings = embeddings\n",
    "    vis_chunks = chunks\n",
    "\n",
    "# t-SNE reduction to 2D\n",
    "print(\"ğŸ”„ Running t-SNE (this may take a minute)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "coords_2d = tsne.fit_transform(vis_embeddings)\n",
    "\n",
    "# Color by primary category\n",
    "categories = [c.get('categories', ['unknown'])[0] if c.get('categories') else 'unknown' for c in vis_chunks]\n",
    "unique_cats = list(set(categories))\n",
    "cat_to_color = {cat: i for i, cat in enumerate(unique_cats)}\n",
    "colors = [cat_to_color[cat] for cat in categories]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    coords_2d[:, 0], coords_2d[:, 1],\n",
    "    c=colors, cmap='tab20', alpha=0.6, s=15,\n",
    "    edgecolors='none',\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "handles = []\n",
    "for cat in unique_cats[:10]:  # Show top 10 categories\n",
    "    color = plt.cm.tab20(cat_to_color[cat] / len(unique_cats))\n",
    "    handles.append(plt.scatter([], [], c=[color], label=cat, s=50))\n",
    "ax.legend(handles=handles, title='Category', loc='upper right', fontsize=8)\n",
    "\n",
    "ax.set_title('t-SNE Visualization of Paper Chunk Embeddings', fontsize=14)\n",
    "ax.set_xlabel('t-SNE Dimension 1')\n",
    "ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / 'data' / 'embedding_tsne.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… t-SNE plot saved to data/embedding_tsne.png\")\n",
    "print(f\"   Visualized {len(vis_embeddings)} chunks across {len(unique_cats)} categories\")\n",
    "\n",
    "# Retrieval heatmap for test queries\n",
    "print(\"\\nğŸ“Š Generating retrieval score heatmap...\")\n",
    "\n",
    "heatmap_queries = [\n",
    "    \"transformer architecture\",\n",
    "    \"attention mechanism\",\n",
    "    \"image classification CNN\",\n",
    "    \"language model pretraining\",\n",
    "    \"reinforcement learning\",\n",
    "]\n",
    "\n",
    "heatmap_data = []\n",
    "for q in heatmap_queries:\n",
    "    results = semantic_search(q, top_k=10)\n",
    "    scores = [r['score'] for r in results]\n",
    "    heatmap_data.append(scores)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    xticklabels=[f'Rank {i+1}' for i in range(10)],\n",
    "    yticklabels=[q[:30] for q in heatmap_queries],\n",
    "    annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('Retrieval Similarity Scores Heatmap')\n",
    "ax.set_xlabel('Retrieval Rank')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(PROJECT_ROOT / 'data' / 'retrieval_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Heatmap saved to data/retrieval_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12: Summary & Future Work\n",
    "\n",
    "Key takeaways, limitations, and potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  Cell 12: Summary & Future Work\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Pipeline summary\n",
    "summary = {\n",
    "    \"Papers Processed\": len(documents),\n",
    "    \"Total Chunks\": len(chunks),\n",
    "    \"Embedding Model\": EMBEDDING_MODEL_NAME,\n",
    "    \"Embedding Dimension\": engine.dimension,\n",
    "    \"Chunk Size\": CHUNK_SIZE,\n",
    "    \"Chunk Overlap\": CHUNK_OVERLAP,\n",
    "    \"FAISS Index Vectors\": store.index.ntotal,\n",
    "    \"Avg Retrieval Latency\": f\"{np.mean(latencies):.1f}ms\",\n",
    "    \"Avg Top-1 Score\": f\"{np.mean(all_top1_scores):.4f}\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"â•\"*60)\n",
    "print(\"  ğŸ“‹ RAG PIPELINE SUMMARY\")\n",
    "print(\"â•\"*60)\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key:30s}: {value}\")\n",
    "\n",
    "print(f\"\\n\\n{'â•'*60}\")\n",
    "print(\"  ğŸ”® FUTURE IMPROVEMENTS\")\n",
    "print(f\"{'â•'*60}\")\n",
    "\n",
    "improvements = [\n",
    "    (\"Hybrid Search\", \"Combine dense (semantic) + sparse (BM25) retrieval for better precision\"),\n",
    "    (\"Re-ranking\", \"Add a cross-encoder re-ranker (e.g., ms-marco-MiniLM) on top-k results\"),\n",
    "    (\"Metadata Filtering\", \"Filter by category, date, or author before/after retrieval\"),\n",
    "    (\"Multi-Query\", \"Generate multiple query variations for broader recall\"),\n",
    "    (\"Chunk Deduplication\", \"Remove near-duplicate chunks using MinHash or cosine threshold\"),\n",
    "    (\"Evaluation Framework\", \"Use RAGAS or DeepEval for automated RAG evaluation metrics\"),\n",
    "    (\"Streaming Generation\", \"Stream LLM responses for better UX in the Streamlit app\"),\n",
    "    (\"Citation Linking\", \"Link answers back to specific pages/sections in the source PDFs\"),\n",
    "]\n",
    "\n",
    "for title, desc in improvements:\n",
    "    print(f\"\\n  ğŸ”¹ {title}\")\n",
    "    print(f\"     {desc}\")\n",
    "\n",
    "print(f\"\\n\\n{'â•'*60}\")\n",
    "print(\"  âœ… Pipeline complete! Run the Streamlit app for an interactive UI:\")\n",
    "print(\"     streamlit run app/streamlit_app.py\")\n",
    "print(f\"{'â•'*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
